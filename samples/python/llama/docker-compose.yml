version: '3.9'
services:
  llama:
    build:
      context: .
      dockerfile: Dockerfile
      shm_size: 30gb
    deploy:
      resources:
        reservations:
          cpus: '2.0'
          memory: 4096M
          devices:
            - capabilities: ["gpu"]
    ports:
      - mode: ingress
        target: 5000
